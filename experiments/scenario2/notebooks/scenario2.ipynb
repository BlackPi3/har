{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "623d7513",
   "metadata": {},
   "source": [
    "# Scenario 2 — Modular experiment notebook\n",
    "\n",
    "This notebook runs scenario 2 training pipeline. It uses the modular codebase: `src.config`, `src.data`, and `src.train`.\n",
    "Use the CLI runner `run_experiment.py` for reproducible command-line runs; this notebook is mainly for quick debugging and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802ca796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.config import load_config, set_seed\n",
    "from src.data import get_dataloaders\n",
    "from src.train import Trainer\n",
    "from src.models import Regressor, FeatureExtractor, ActivityClassifier\n",
    "\n",
    "# Notebook-friendly plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70214038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n",
      "Loaded config from ../configs/scenario2.yaml\n",
      "dataset_name: mmfit\n",
      "data_dir: ../../../data/mm-fit/\n",
      "train_subjects: ['w01', 'w02', 'w03', 'w04', 'w06', 'w07', 'w08', 'w16', 'w17', 'w18', 'w00', 'w05']\n"
     ]
    }
   ],
   "source": [
    "# Load merged config (base + experiment) and optionally override small values here\n",
    "base_cfg = \"../../../configs/base.yaml\"  # Fixed path relative to experiments/scenario2/notebooks/\n",
    "exp_cfg = \"../configs/scenario2.yaml\"    # Config is now in experiment directory\n",
    "opts = []  # e.g. [alpha=1.0]\n",
    "\n",
    "cfg = load_config(base_cfg, exp_cfg, opts=opts)\n",
    "set_seed(getattr(cfg, 'seed', None))\n",
    "print('device:', cfg.device)\n",
    "print('Loaded config from', exp_cfg)\n",
    "print('dataset_name:', getattr(cfg, 'dataset_name', 'NOT_FOUND'))\n",
    "print('data_dir:', getattr(cfg, 'data_dir', 'NOT_FOUND'))\n",
    "print('train_subjects:', getattr(cfg, 'train_subjects', 'NOT_FOUND'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaa82a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets sizes: {'train': 8400, 'val': 2800, 'test': 2800}\n"
     ]
    }
   ],
   "source": [
    "# Create dataloaders from factory\n",
    "dls = get_dataloaders(cfg.dataset_name, cfg)  # expected keys: 'train','val','test'\n",
    "print('Datasets sizes:', {k: len(v.dataset) for k, v in dls.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d522864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts: {'pose2imu': 4408836, 'fe': 68626, 'ac': 1111}\n"
     ]
    }
   ],
   "source": [
    "# Build models from src.models using cleaner config structure\n",
    "device = getattr(cfg, 'torch_device', torch.device(cfg.device))\n",
    "\n",
    "models = {\n",
    "    'pose2imu': Regressor(\n",
    "        in_ch=cfg.models.regressor.input_channels,\n",
    "        num_joints=cfg.models.regressor.num_joints,\n",
    "        window_length=cfg.models.regressor.sequence_length,\n",
    "    ).to(device),\n",
    "    'fe': FeatureExtractor().to(device),\n",
    "    'ac': ActivityClassifier(\n",
    "        f_in=cfg.models.classifier.f_in, \n",
    "        n_classes=cfg.models.classifier.n_classes\n",
    "    ).to(device),\n",
    "}\n",
    "\n",
    "def count_params(m):\n",
    "    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "print('param counts:', {k: count_params(m) for k, m in models.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13bb7320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer instantiated\n"
     ]
    }
   ],
   "source": [
    "# Optimizer, scheduler and Trainer\n",
    "# Convert lr to float if it's a string (YAML sometimes loads 1e-3 as string)\n",
    "lr = float(cfg.lr) if isinstance(cfg.lr, str) else cfg.lr\n",
    "\n",
    "params = sum([list(m.parameters()) for m in models.values()], [])\n",
    "optimizer = torch.optim.Adam(params, lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=cfg.patience)\n",
    "\n",
    "trainer = Trainer(models=models, dataloaders=dls, optimizer=optimizer, scheduler=scheduler, cfg=cfg, device=device)\n",
    "print('Trainer instantiated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b6b66b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 5 epochs...\n",
      "----------------------------------------------------------------------\n",
      "Epoch   1/5 | Train Loss: 0.9624 | Val Loss: 5.0663 | Train F1: 0.1897 | Val F1: 0.5527\n",
      "    → New best Val F1: 0.5527\n",
      "Epoch   2/5 | Train Loss: 0.3295 | Val Loss: 4.4051 | Train F1: 0.9864 | Val F1: 0.5096\n",
      "Epoch   3/5 | Train Loss: 0.2603 | Val Loss: 8.2558 | Train F1: 0.9877 | Val F1: 0.5256\n",
      "Epoch   4/5 | Train Loss: 0.2542 | Val Loss: 7.1137 | Train F1: 0.9917 | Val F1: 0.4488\n",
      "Epoch   5/5 | Train Loss: 0.1995 | Val Loss: 6.8094 | Train F1: 0.6611 | Val F1: 0.4627\n",
      "----------------------------------------------------------------------\n",
      "Training completed. Best Val F1: 0.5527\n",
      "Training finished. History keys: ['train_loss', 'val_loss', 'train_f1', 'val_f1']\n"
     ]
    }
   ],
   "source": [
    "# ORIGINAL TRAINING CELL - Use this one, not the debug cell below\n",
    "# Run training (for quick notebook tests, override epochs in cfg or pass a small number)\n",
    "# For testing, you can use: history = trainer.fit(1)\n",
    "# For full training, use: history = trainer.fit(cfg.epochs)\n",
    "epochs_to_run = 5  # Change this to cfg.epochs for full training\n",
    "history = trainer.fit(epochs_to_run)\n",
    "print('Training finished. History keys:', list(history.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09243fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple visualizations\n",
    "plt.figure(figsize=(7,4))\n",
    "if 'train_loss' in history and 'val_loss' in history:\n",
    "    plt.plot(history['train_loss'], label='train_loss')\n",
    "    plt.plot(history['val_loss'], label='val_loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No loss history in trainer output')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383a4c28",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- This notebook follows the modular approach: it loads a merged config, builds dataloaders, models, and delegates training to `src.train.Trainer`.\n",
    "- After you verify this notebook reproduces expected behaviour, delete or archive the old `legacy/scenario2_legacy.ipynb`.\n",
    "- For reproducible CLI runs use: `python run_experiment.py --experiment scenario2 --config configs/scenario2.yaml --seed 42`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "har",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
