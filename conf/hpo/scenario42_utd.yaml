# HPO for Scenario42 (signal-level adversarial) on UTD
# scenario42 = scenario2 + signal-level discriminator (D on raw acc)
# Unlike scenario4 (D on features), D directly trains the regressor
#
# Multi-pass HPO Strategy:
# - Pass 1: Loss weights + Adversarial params (most impactful)
# - Pass 2: Regularization (after fixing pass 1 winners)
# - Pass 3: Capacity (after fixing pass 1+2 winners)

study_name: scenario42_utd
trial: scenario42_utd

# HPO-specific overrides (shorter epochs for faster search)
trainer:
  epochs: 50
  patience: 25

top_k: 10
save_artifacts: false  # Save plots to monitor D_acc and feat_dist

repeat:
  enabled: true
  count: 3

hydra:
  sweeper:
    study_name: ${study_name}
    storage: "sqlite:///experiments/hpo/${study_name}/${study_name}.db"
    params:

      # ============================================================
      # PASS 1: Loss weights + Adversarial (ACTIVE)
      # ============================================================
      # trainer.alpha: choice(0.1, 0.5, 1, 2)
      # trainer.beta: choice(1, 2, 5, 10, 50, 100)
      # trainer.gamma: choice(1, 2, 5)

      # trainer.adversarial.weight: choice(0.1, 0.5, 1.0, 2, 5)
      # trainer.adversarial.n_critic: choice(1, 3, 5)            # D updates per G update
      # trainer.adversarial.discriminator.aux_weight: choice(0.5, 1.0, 2.0)
      # trainer.adversarial.pretrain_epochs: choice(0, 5, 10, 15)   # MSE-only warmup

      # data.stride_seconds: choice(0.1)
      # data.batch_size: choice(16)

      # ============================================================
      # PASS 2: Regularization (UNCOMMENT after Pass 1)
      # ============================================================
      # # Fix Pass 1 winners, then search regularization
      # trainer.alpha: choice(0.5)
      # trainer.beta: choice(10)
      # trainer.gamma: choice(2)

      # trainer.adversarial.weight: choice(1)
      # trainer.adversarial.n_critic: choice(5)            # D updates per G update
      # trainer.adversarial.pretrain_epochs: choice(10)   # MSE-only warmup
      # trainer.adversarial.discriminator.aux_weight: choice(1)

      # data.stride_seconds: choice(0.1)
      # data.batch_size: choice(16)
      
      # # Search regularization:
      # optim.lr: tag(log, interval(3e-4, 2e-3))
      # optim.weight_decay: tag(log, interval(1e-5, 1e-4))
      # optim.warmup_epochs: choice(0, 5, 10)
      # optim.scheduler.patience: choice(10, 15, 20)
      # optim.scheduler.factor: choice(0.05, 0.1, 0.2)
      # trainer.label_smoothing: choice(0, 0.05, 0.1)
      # model.encoder_classifier.feature_extractor.drop_prob: choice(0.1, 0.2, 0.3, 0.4)
      # model.encoder_classifier.classifier.dropout: choice(0.2, 0.3, 0.4, 0.5, 0.6)
      # model.regressor.joint_dropouts: choice([0.0, 0.2, 0.2, 0.2], [0.2,0.3,0.3,0.3], [0.3,0.4,0.4,0.4], [0.4,0.5,0.5,0.5])
      # model.regressor.temporal_dropout: choice(0.1, 0.2, 0.3, 0.4)
      # trainer.adversarial.discriminator.dropout: choice(0.1, 0.2, 0.3, 0.4)
      # trainer.adversarial.discriminator.label_smoothing: choice(0, 0.05, 0.1)

      # ============================================================
      # PASS 3: Capacity (UNCOMMENT after Pass 2)
      # ============================================================
      # Fix Pass 1+2 winners, then search model capacity
      trainer.alpha: choice(0.5)
      trainer.beta: choice(10)
      trainer.gamma: choice(2)
      trainer.adversarial.weight: choice(1)
      trainer.adversarial.n_critic: choice(5)            # D updates per G update
      trainer.adversarial.pretrain_epochs: choice(10)   # MSE-only warmup
      trainer.adversarial.discriminator.aux_weight: choice(1)
      data.stride_seconds: choice(0.1)
      data.batch_size: choice(16)
      # Search regularization:
      optim.lr: choice(0.00046285956125273193)
      optim.weight_decay: choice(1.1475981059945094e-05)
      optim.warmup_epochs: choice(0)
      optim.scheduler.patience: choice(15)
      optim.scheduler.factor: choice(0.2)
      trainer.label_smoothing: choice(0.05)
      model.encoder_classifier.feature_extractor.drop_prob: choice(0.1)
      model.encoder_classifier.classifier.dropout: choice(0.5)
      model.regressor.joint_dropouts: choice([0.0, 0.2, 0.2, 0.2])
      model.regressor.temporal_dropout: choice(0.4)
      trainer.adversarial.discriminator.dropout: choice(0.1)
      trainer.adversarial.discriminator.label_smoothing: choice(0.1)

      model.encoder_classifier.feature_extractor.kernel_size: choice(7, 9, 11)
      model.encoder_classifier.feature_extractor.base_filters: choice([6,9,12], [8,12,16], [9,15,24])
      model.encoder_classifier.feature_extractor.embedding_dim: choice(80, 100, 120)
      model.encoder_classifier.classifier.hidden_units: choice([64], [100], [128])
      model.regressor.joint_hidden_channels: choice([16, 16, 16, 16], [24, 24, 24, 24], [32, 32, 32, 32])
      model.regressor.temporal_hidden_channels: choice(8, 12, 16)
      model.regressor.joint_kernel_sizes: choice([3,3,3,3], [5,5,5,5])

      trainer.adversarial.discriminator.hidden_channels: choice([16, 32], [32, 64], [64, 128])
      trainer.adversarial.discriminator.embed_dim: choice(16, 32, 64)

  sweep:
    dir: "experiments/hpo/${study_name}/trials"
    subdir: trial_${hydra.job.num}
