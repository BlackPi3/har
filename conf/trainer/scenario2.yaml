## Manual Trainer configuration for scenario2 (pose2imu + HAR multitask)
defaults:
  - _self_

# todo add weights here later

# Number of epochs/patience used by experiments.run_trial Trainer wrapper
epochs: 100
patience: 25  # early-stopping tolerance (epochs without val improvement)
gradient_clip: 1.0  # set to a positive value (e.g., 1.0) to enable grad clipping

alpha: 1.0  # activity loss weight
beta: 1.0   # feature similarity weight
gamma: 1.0  # regression (MSE) weight
label_smoothing: 0.0  # label smoothing epsilon for activity loss

separate_classifiers: false  # when true, use separate classifier for sim stream
separate_feature_extractors: false  # when true, use a separate feature extractor for sim stream

objective:
  metric: val_f1   # Any key from Trainer history (e.g., val_f1, val_mse, val_loss)
  mode: max        # max to maximize, min to minimize (e.g., use min for val_mse)

# Select which sub-modules receive optimizer updates.
# Modules set to false are frozen (no optimizer params) but still run during forward passes.
trainable_modules:
  pose2imu: true
  fe: true
  ac: true

# Fine-grained loss toggles. Each flag gates the corresponding component regardless of alpha/beta.
losses:
  mse: true
  feature_similarity: true
  activity: true       # Master switch for activity losses
  activity_real: true  # Cross-entropy on real accelerometer branch
  activity_sim: true   # Cross-entropy on simulated accelerometer branch

# Optional secondary pose-only stream (e.g., NTU). Disabled by default.
secondary:
  enabled: false
  loss_weight: 1.0
  data: ntu50.yaml  # name of a config under conf/data (e.g., ntu50.yaml or ntu100.yaml)
  n_classes: 60

# Adversarial training (Scenario 4): feature-level discriminator with GRL.
# Enforces that simulated features are indistinguishable from real features.
adversarial:
  enabled: false
  weight: 0.1           # weight of adversarial loss in total loss
  use_grl: true         # use Gradient Reversal Layer (recommended for domain adaptation)
  grl_lambda: 1.0       # GRL strength (fixed or initial value if scheduled)
  schedule_lambda: false  # if true, ramp lambda from 0â†’1 over training (DANN-style)
  schedule_gamma: 10.0  # steepness of lambda schedule (only used if schedule_lambda=true)
  # For future extension: alternating D/G updates instead of GRL
  alternating: false    # NOT YET IMPLEMENTED - use GRL for now
  n_critic: 1           # D updates per G update (only for alternating mode)
  discriminator:
    hidden_units: [64]  # MLP hidden layer sizes
    dropout: 0.3
