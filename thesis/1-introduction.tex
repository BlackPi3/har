\chapter{Introduction}

% Background and Context
Human Activity Recognition (HAR) refers to the task of automatically identifying and classifying physical activities, such as swinging tennis bat, doing push-ups, or throwing a ball, using data collected from various sensors. These sensors include Inertial Measurement Units (IMUs), which typically consist of accelerometers and gyroscopes, as well as video cameras, depth sensors, and audio devices. HAR is central to enabling context-aware computing, wherein systems adapt to the user's activity and environment in real-time.

Initially, HAR systems relied on hand-crafted features and classical machine learning techniques such as Decision Trees, Support Vector Machines, and Random Forests \cite{lara2012survey}. However, the advent of deep learning, particularly convolutional neural networks (CNNs) and recurrent neural networks (RNNs), has dramatically improved performance by enabling automatic feature extraction from raw sensor data \cite{hammerla2016deep, wang2019deep}. More recently, attention-based models and transformers have further enhanced sequential data modeling capabilities \cite{zeng2018understanding}.

Key concepts in HAR include:
\begin{itemize}
    \item \textbf{Sensor Modalities:} Different types of sensors (e.g. IMU, RGB video, audio) used to capture activity data.
    \item \textbf{Temporal Dynamics:} The importance of sequence and time dependency in accurate recognition of activities.
    \item \textbf{Feature Extraction:} Techniques to transform raw sensor data into meaningful representations, by handcrafted or learned features.
    \item \textbf{Multi-modal Data:} Combining multiple sensor sources to improve recognition accuracy and robustness.
\end{itemize}

HAR plays a significant role across diverse domains:
\begin{itemize}
    \item In healthcare, it facilitates patient monitoring, rehabilitation assessment, and fall detection \cite{yoshida2022data, sangeethalakshmi2023patient}.
    \item In fitness and sports, HAR systems enable personalized training, movement analysis, and injury prevention.
    \item In industry, HAR supports ergonomic risk detection, workplace safety automation, and productivity monitoring.
\end{itemize}
These applications highlight the social and economic value of accurate HAR systems and its growing integration into smart environments \cite{hammerla2016deep, wang2019deep}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Problem statement
Despite progress in deep learning models, such as Convolutional Neural Networks (CNNs), Long Short-Term Memory networks (LSTMs), and transformers, Human Activity Recognition (HAR) remains bottlenecked by the scarcity of labeled data. Collecting and annotating real-world sensor data is both expensive and time-consuming, requiring diverse participants, varied environments, and meticulous labeling. Consequently, state-of-the-art HAR models often suffer from poor generalization to unseen users, novel activities, or differing sensor configurations, significantly constraining broader deployment in real-world settings.

This limitation is especially critical given the growing reliance on HAR in domains like healthcare, industrial safety, and fitness monitoring. In healthcare, for example, poor model generalization can lead to misclassifications that compromise patient monitoring and fall detection systems \cite{yoshida2022data}. Similarly, in industrial settings, unreliable HAR can hinder automation aimed at ensuring worker safety and ergonomics.

Addressing the data scarcity challenge is therefore pivotal not only for improving model accuracy and robustness, but also for enabling the reliable, scalable application of HAR technologies in everyday and high-stakes environments.

Researchers have proposed several approaches to mitigate this challenge:

\begin{itemize}
    \item \textbf{Data augmentation}: Techniques such as noise injection, time-warping, rotation, and scaling artificially expand training datasets, though they may lack the semantic richness needed for complex activities \cite{um2017data}.
    \item \textbf{Generative models}: GANs and diffusion models synthesize realistic sensor data, introducing richer variability in activity patterns \cite{li2024diffusionhar}.
    \item \textbf{Cross-modal synthetic data generation}:
    \begin{itemize}
        \item \textit{Video-to-IMU pipelines} (e.g. IMUTube) generate synthetic IMU signals from large-scale video datasets, capturing realistic temporal and kinematic dynamics \cite{kwon2020imutube}.
        \item \textit{Language-to-IMU pipelines} (e.g. IMUGPT) convert textual activity descriptions into motion data, producing diverse sensor signals with semantic variety \cite{leng2024imugpt}.
    \end{itemize}
\end{itemize}

These advanced methods have demonstrated substantial improvements in HAR performance under limited data regimes, often surpassing traditional augmentation strategies. Nevertheless, challenges persist in ensuring temporal consistency, modality alignment, and generalizability of the generated data, especially to other subjects, areas that this thesis seeks to explore further.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Research Questions or Hypotheses
Given the challenges of data scarcity in HAR and the opportunities presented by multi-modal data, this thesis investigates several interrelated questions aimed at enhancing HAR performance, on well known datasets such as MMFit, UTD-MHAD and NTU RGB+D \cite{stromback2020mm, chen2015utd, shahroudy2016ntu}, through novel training pipelines.

A primary objective of this work is to explore whether HAR performance can be improved by integrating a joint training framework that concurrently learns a sensor data regressor and an activity classifier. Specifically, this involves generating synthetic wrist accelerometer data from corresponding joint pose sequences of an arm, comprising the wrist, elbow, and shoulder, and utilizing both real and generated sensor data to inform the classifier. This approach differs from prior works where regressors and classifiers were trained separately, and we hypothesize that concurrent optimization may yield more robust activity representations.

Another key investigation concerns the role of loss functions in this pipeline. We do ablation studies to understand the role of each loss in our pipeline. For instance in one study, we examine whether removing mean squared error (MSE) loss between real and simulated sensor data, affects the classifier’s effectiveness. Understanding the contribution of each loss component is critical for designing efficient HAR models.

Additionally, this thesis explores architectural variants that use separate feature extractors or separate classifiers for real and simulated data paths. These variants test whether domain-specific components improve performance compared to fully shared architectures.

We also investigate the potential of leveraging external pose datasets, specifically NTU RGB+D \cite{shahroudy2016ntu}, to supplement the pose-to-sensor data generation process. By incorporating pose data from a broader range of activities, we aim to assess whether cross-dataset augmentation can further enhance IMU-based HAR performance, particularly under low-data regimes.

Finally, we explore the introduction of adversarial components into the training pipeline. We investigate two complementary approaches: (1) feature-level discrimination using a Gradient Reversal Layer (GRL) that encourages the feature extractor to produce domain-invariant representations, and (2) signal-level discrimination using Wasserstein GAN with gradient penalty (WGAN-GP) that trains the regressor to produce realistic accelerometer waveforms through alternating discriminator/generator updates. We analyze the adversarial learning dynamics to understand how the competition between generator and discriminator affects classification accuracy. 

Through these inquiries, this thesis seeks to advance understanding of how synthetic data generation, multi-modal learning, loss function design, and adversarial training interact to address data scarcity and enhance the generalization capabilities of HAR systems.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Research Objectives

The overarching objective of this thesis is to address the data scarcity problem in HAR by leveraging multi-modal data and advanced training strategies. To this end, the specific objectives of this research are:

\begin{enumerate}
    \item To design and implement a unified training pipeline that concurrently trains a pose-to-IMU regressor and an activity classifier, integrating real and synthetic accelerometer data derived from joint pose sequences, evaluated on MM-Fit and UTD-MHAD datasets.

    \item To investigate the effect of different loss function configurations, specifically evaluating the contribution of MSE regression loss and feature similarity loss through systematic ablation studies.

    \item To explore architectural variants including separate feature extractors and separate classifiers for real and simulated data paths, testing whether domain-specific components improve over fully shared architectures.

    \item To augment the pose-to-sensor data generation process with NTU RGB+D as a secondary pose-only dataset, evaluating whether additional pose diversity improves HAR performance.

    \item To integrate adversarial discriminators into the training pipeline using two approaches: feature-level discrimination with gradient reversal for domain-invariant representations, and signal-level discrimination with WGAN-GP for realistic signal synthesis.
\end{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Scope and Limitations

This thesis focuses on addressing the data scarcity problem in HAR by leveraging multi-modal data, specifically wrist accelerometer data and upper-limb joint pose data (wrist, elbow, shoulder). The study primarily utilizes publicly available datasets: MM-Fit and UTD-MHAD as primary datasets with synchronized pose and accelerometer data, and NTU RGB+D as a secondary pose-only dataset for auxiliary training experiments.

The research explores the design and evaluation of a joint training pipeline that concurrently trains a pose-to-IMU regressor and an activity classifier. Eight training scenarios are systematically compared through ablation studies, exploring loss function configurations, architectural variants (shared vs.\ separate components), auxiliary data integration, and adversarial training approaches. Experiments focus on evaluating macro F1 score and accuracy, with subject-disjoint evaluation to assess cross-subject generalization.

However, this research is subject to several limitations. The work is constrained to the selected datasets and may not generalize across all types of HAR data or sensor configurations, such as gyroscope or magnetometer readings. The experiments are conducted in an offline setting without addressing real-time inference constraints or optimization for deployment on resource-constrained devices. The synthetic data generation is activity-specific and may not generalize well to activities not represented in the training datasets. Furthermore, the scope is limited to upper-limb joint poses; comprehensive modeling of full-body activities remains outside the current research focus. The evaluation uses an ablation-based design rather than comparison against external baseline methods.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Thesis Structure

The remainder of this thesis is organized as follows:

\begin{itemize}
    \item \textbf{Chapter 2: Related Work} reviews relevant literature on Human Activity Recognition, focusing on data scarcity challenges, data augmentation methods, cross-modal synthetic data generation, and adversarial learning for domain adaptation.

    \item \textbf{Chapter 3: Methodology} presents the proposed framework, detailing the system architecture with its three main components (regressor, feature extractor, classifier), the loss function design, and the training scenarios explored.

    \item \textbf{Chapter 4: Implementation} describes the neural network architectures, the Hydra-based configuration system, the unified trainer supporting all scenarios, and the multi-pass hyperparameter optimization strategy.

    \item \textbf{Chapter 5: Experimental Setup} details the datasets (UTD-MHAD, MM-Fit, NTU RGB+D), preprocessing procedures, evaluation protocols, and the eight experimental scenarios used in the ablation-based evaluation.

    \item \textbf{Chapter 6: Results} presents the experimental findings, including performance comparisons across scenarios, ablation study results, and analysis of adversarial training dynamics.

    \item \textbf{Chapter 7: Discussion} interprets the findings, discusses when simulation helps, analyzes the adversarial learning insights, acknowledges limitations, and provides practical recommendations.

    \item \textbf{Chapter 8: Conclusion} summarizes the contributions, answers the research questions, and outlines directions for future work.
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% In this thesis, we introduce a novel adversarial learning approach to enhance wearable sensor-based Human Activity Recognition (HAR) by leveraging synthetic sensor data generation. Specifically, we propose two approaches, with the core innovation being the introduction of an adversarial framework that significantly outperforms a standard end-to-end pipeline without a discriminator.

% The first approach involves a pose-to-sensor network that generates IMU sensor data directly from 3D skeleton pose sequences. Unlike traditional methods, this model jointly trains the pose-to-sensor network and the HAR classifier. The pose-to-sensor network is optimized to minimize both the reconstruction loss between the ground-truth sensor data and the generated sensor data, as well as the classification loss of the HAR classifier using both real and synthetic data. This simultaneous training process allows the generated synthetic sensor data to closely mimic real sensor data while directly improving the performance of the activity classifier. While this end-to-end pipeline shows performance gains by leveraging the relationship between sensor data generation and activity recognition, it remains limited in its ability to fully capture the complexity of real-world sensor distributions.

% To address these limitations, we introduce an adversarial learning scheme in the second approach, which significantly enhances the generation of realistic sensor data. This method incorporates a discriminator network into the architecture, which distinguishes between real and generated sensor data.

% To evaluate the proposed methods, we evaluate the proposed method on the well-known open-access benchmark datasets, MM-Fit \cite{stromback2020mm}, UTD-MHAD \cite{chen2015utd}, NTURGB+D \cite{shahroudy2016ntu} and NTURGB+D 120 \cite{liu2019ntu} which provide multimodal data including the skeleton, IMU, and label data, except for NTURGB+D which does not provide IMU data, enabling the synthesis of realistic and diverse training samples. Experimental results demonstrate that the proposed framework provides better performance improvement in terms of accuracy and macro F1 score compared to existing methods, including IMUTube \cite{kwon2020imutube}, Chen et al. \cite{chen2015utd}, and Memmesheimer et al. \cite{memmesheimer2020gimme}, and baseline which trains the pose-to-sensor network model without considering the classifier. 

% The contributions of this thesis build on a previous publication \cite{zolfaghari2024sensor}, where we developed a pose-to-sensor network for generating synthetic sensor data. This thesis advances that work by integrating adversarial learning, resulting in improved synthetic data realism and better activity classification performance.
% The main contributions of this thesis are summarized as follows:

% \begin{itemize}
%     \item We propose an adversarial learning framework for Human Activity Recognition (HAR) that enhances the performance of an activity classifier by integrating a discriminator network. The discriminator distinguishes between real accelerometer data from the primary dataset and synthetic accelerometer data generated by a pose-to-sensor network. This adversarial scheme forces the generator to produce high-quality sensor data, leading to improved classification accuracy.
%     \item Unlike conventional end-to-end pipelines, the adversarial approach leverages a pre-trained pose-to-sensor network to initialize the generator. This network is trained on the primary dataset to generate realistic sensor data from 3D skeletal poses. The inclusion of a discriminator significantly boosts the quality of the generated data and enables the feature extractor to learn more discriminative features, resulting in better generalization and robustness in activity recognition.
%     \item The adversarial scheme is compared against a baseline model that uses the same network architecture without the discriminator. The results show that the adversarial model achieves superior performance in terms of activity classification accuracy.
%     \item Extensive evaluations are conducted using the UTD-MHAD \cite{chen2015utd}, NTU-RGB+D \cite{shahroudy2016ntu}, and MM-Fit \cite{liu2019ntu} datasets, demonstrating consistent improvements in HAR performance over the baseline model.
% \end{itemize}


% The remainder of this thesis is organized as follows: \cref{chap:relatedworks} reviews related work improving HAR performance by generating IMU sensor data augmentation. \cref{chap:method} describes the methodology, including the data synthesis process and the end-to-end training pipeline. \cref{chap:experiments} presents the experimental setup, results, and comparative analysis. Finally, \cref{chap:conclusion} concludes the paper with a summary of our findings and a discussion of potential future work.


% Human Activity Recognition (HAR) has become a fundamental technology across various domains, including personal fitness, healthcare, and industrial automation. The widespread adoption of smart wearable devices, such as smartwatches and fitness trackers, has revolutionized the ability to monitor physical activities in real-time. These devices, equipped with sophisticated sensors like Inertial Measurement Units (IMUs), generate vast amounts of data that offer valuable insights into user behavior. When effectively analyzed, this data can be leveraged to provide tailored health and fitness recommendations, enhancing the user’s ability to achieve their wellness and performance goals. Despite this potential, the challenge remains in harnessing the full capability of deep learning models due to the scarcity of labeled data required for robust activity classification. This underscores the importance of augmenting existing sensor data to improve the accuracy and reliability of HAR systems in real-world applications.

% Beyond fitness, the implications of precise HAR are profound, extending into elderly care for fall detection \cite{yoshida2022data}, patient monitoring in healthcare \cite{sangeethalakshmi2023patient}, and even worker safety in manufacturing line \cite{suh2023worker}. Accurate activity recognition facilitates the development of intelligent systems that are responsive to human needs, enhancing safety, productivity, and overall quality of life.

% Despite its vast potential, Human Activity Recognition (HAR) faces significant challenges, primarily due to the limited availability of labeled sensor data. Unlike computer vision tasks, where large annotated datasets are readily available, HAR struggles with a severe bottleneck in obtaining the extensive annotated sensor data needed to train accurate and generalizable models. The process of manually labeling sensor data for diverse and nuanced human activities is not only time-consuming and labor-intensive but also prohibitively expensive, particularly as the complexity of activities increases. Moreover, many HAR systems rely on deploying sensors across multiple body parts to capture detailed motion information, further complicating the data collection process and increasing both the logistical burden and cost of annotation. These challenges underscore the importance of developing data-efficient methods, such as adversarial data augmentation, to mitigate the reliance on large labeled datasets while maintaining or even enhancing recognition accuracy.

% To address these challenges, researchers have explored alternative avenues. Data augmentation stands as a powerful technique to combat the dearth of labeled data, enhancing the diversity and volume of training datasets, and thereby improving the performance of machine learning models. In scenarios where data is scarce or expensive to obtain, augmentation has proven to be effective, particularly in image and speech recognition tasks. In particular, recently, several sensor data generation methods from video sequences have emerged in the HAR community. Existing works \cite{kwon2020imutube, fortes2021translating, santhalingam2023synthetic} estimate 2D or 3D joint positions from videos and infer joint orientations to compute inertial measurement unit (IMU) data. While these methods improved the spatial accuracy of generated sensor data and enhanced the performance of wearable sensor-based HAR, they still struggle with capturing the intricacies of sensor characteristics and may not fully exploit the potential for cross-modality transfer. 