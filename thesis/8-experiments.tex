\chapter{Experimental Setup}
\label{chap:experimental_setup}

This section provides a comprehensive overview of the empirical evaluation conducted to assess the effectiveness of our proposed method. We commence by detailing the MM-Fit dataset, which serves as the foundational data source for our study. Following this, we outline the baseline method against which we benchmark our approach, establishing a context for comparative analysis.

This section culminates with a presentation of the results derived from our evaluation, illustrating the impact of our modifications and the overall performance of our proposed method in the context of Human Activity Recognition (HAR).

\section{Dataset}
The MM-Fit dataset forms the core of our study, tracking participants engaged in a variety of workout activities. It provides a comprehensive suite of data capturing 2D and 3D skeletal poses and Inertial Measurement Unit (IMU) sensor readings from the wrist and other body locations across 21 workout sessions. Each session is composed of three sets, with each set containing ten exercises and ten repetitions. During the rest intervals between sets, the sensors continue to record, capturing the natural rest behavior of participants. The dataset categorizes eleven different activities, including periods of 'no workout'.

The UTD-MHAD dataset offers a comprehensive collection of 27 diverse human actions that includes 20 upper body and 7 lower body motions, ranging from simple gestures like hand waves and clapping to complex movements like basketball shooting and jogging. A Kinect camera and wearable inertial sensor is used for data collection. The Kinect camera captures high-resolution color and depth images at a frame rate of approximately 30 frames per second, providing detailed visual data. Complementing this, the wearable inertial sensor, positioned either on the subject's wrist or thigh depending on the nature of the action, records precise motion data including acceleration, angular velocity, and magnetic strength at a sampling rate of 50 Hz.

The NTURGB+D dataset contains 120 diverse human actions, including daily activities such as picking up an object, medical-related actions like coughing, and two-person interactions such as hugging. The data was recorded using three Kinect 2 cameras at a rate of 30 frames per second. Unlike other datasets mentioned earlier, NTURGB+D does not provide IMU sensor data from wearable devices. For our work, we utilized only the RGB video data.

In our research, we specifically focus on the 3D skeleton pose data captured from the RGB camera at 30 Hz and the left wrist accelerometer data from a smartwatch sampled at 100 Hz. Following the approach in \cite{fortes2021translating}, our regression model takes as input the three left-arm joints (wrist, elbow, and shoulder).

In the proposed method, we rely solely on the RGB data from the secondary dataset to estimate 3D poses. For this, we use the pre-trained MotionBERT model \cite{zhu2023motionbert}, which is based on 2D keypoints extracted by AlphaPose \cite{fang2022alphapose}, trained on the Halpe dataset, as recommended for in-the-wild inference. The 3D poses generated by MotionBERT are preprocessed in the same way as outlined in \ref{subsec:preprocess}.

Since the NTURGB+D dataset was used as the secondary dataset, it needed to include activity classes common to the UTD-MHAD dataset. For this study, we focused on six activity classes: two-handed front clapping, right-arm throwing, crossing arms over the chest, basketball shooting, tennis forehand swing, and picking up and throwing.

To maintain consistency and comparability with previous studies, we followed the same training, validation, and test splits as those used in the original MM-Fit paper. We tested on a fixed set of unseen users. For data segmentation, we employed 3-second sliding windows (300 samples) with a 0.2-second stride, assigning each window the activity label that occurred most frequently within that window. This setup ensures that our results can be directly compared to established benchmarks in the field.


% ------------------------------------------------------- %

\section{Baselines}

Here we outline three baseline methods used for activity classification, each providing a distinct comparison to evaluate the effectiveness of our proposed adversarial learning approach.

The first baseline method uses only real accelerometer data from the wrist. In this approach, a feature extraction model identifies key features from the accelerometer data, which are then fed into a classifier to predict the activity type. Both the feature extractor and classifier follow the same architecture as in our proposed method. This approach, similar to the one employed in the MM-Fit \cite{stromback2020mm} study, serves as a foundation for comparison, highlighting the improvements achieved by incorporating synthetic sensor data and adversarial learning.

The second baseline involves a two-step process: first, a regression model is trained to generate synthetic sensor data from 3D joint poses, and second, this synthetic data, combined with real sensor data, is used to train the activity classifier. This method aims to augment the training dataset by incorporating synthetic data, with the goal of enhancing classification performance. However, the key limitation of this approach is that the data generation and classification processes are decoupled, meaning the pose-to-sensor network and the classifier are trained separately, which restricts the optimization potential across both tasks.

The third baseline method introduces concurrent training of the pose-to-sensor network and the activity classifier. In this approach, both the sensor data generation and classification tasks are optimized simultaneously. While this method shows improvement over the previous two baselines by integrating sensor generation and classification into a unified process, it lacks the adversarial component, which we demonstrate in our proposed method as crucial for generating higher-quality synthetic sensor data and improving classification performance.

By comparing these three baseline methods, we highlight how our adversarial learning approach—which incorporates a discriminator and optimizes both sensor generation and classification in a competitive framework—significantly improves both the realism of the generated sensor data and the overall activity classification accuracy and F1 score. This comparison underscores the contributions of our work to advancing Human Activity Recognition (HAR).

% ------------------------------------------------------- %

\begin{figure*}[ht!]
    % \vspace{-10pt} % Adjust the negative space as needed
    \centering
    \includegraphics[width=\textwidth]{Thesis_Template_DE_EN/BA_MA_English/Figures/fig2.png}
    \caption{Overview of Baseline Approaches for Activity Classification. The first baseline utilizes only real sensor data (\( \mathbf{x_{\text{sensor}}} \)) for classifier training. The second baseline employs a two-step approach where a regression model (\( R \)) first predicts synthetic sensor data (\( \mathbf{\tilde{x}}_{\text{sensor}} \)) from 3D joint pose sequences (\(\mathbf{x}_{\text{pose}} \)), which is then combined with real data to train the classifier. Both methods converge at the Activity Classifier (\( C \)), which outputs the activity predictions (\( \mathbf{\tilde{y}}_{\text{activity}} \)).}
    \label{fig:my-diagram}
    % \vspace{-10pt} % Adjust the negative space as needed
\end{figure*}

In our approach, we explore the efficacy of integrating both real and synthetic IMU accelerometer data to enhance the training of the feature extraction and classification modules. Initially, we developed a regression model trained independently to accurately predict IMU accelerometer data based on the arm's joint positions, specifically the wrist, elbow, and shoulder. Following the successful generation of synthetic accelerometer data, we merge it with the real sensor data. This combined dataset is then utilized to train the feature extraction and classification modules, aiming to leverage the diversity and comprehensiveness of the augmented data set for improved model performance.

% ------------------------------------------------------- %

\section{Implementation Details}
% \textbf{Initialization and Reproducibility:} 
To ensure a consistent starting point for model training, we initialize the weights of the convolution and fully connected layers using Kaiming initialization. Recognizing the potential impact of initialization randomness, we conducted five runs of the experiment for MMFit and ten runs for UTD-MHAD each with a predefined seed to comprehensively explore the search space.
% \textbf{Optimization and Early Stopping:} 
The Adam optimizer, with a learning rate of $10^{-3}$, was selected for training for all methods including the discriminator. To prevent over-fitting and ensure optimal generalization, we implemented an early stopping mechanism based on the F1 score on the validation set, with a patience parameter of 25 epochs for MMFit and 30 epochs for UTD-MHAD, respectively. The models were trained for a maximum of 100 epochs for MMFit and 200 epochs for UTD-MHAD or until the early stopping criterion was met.

The introduction of early stopping, based on validation set performance, plays a critical role in our experimental design, allowing us to halt training when the model ceases to show improvement, thereby conserving computational resources and avoiding over-fitting.

The results from these experiments highlight the efficacy of our proposed modifications and the robustness of our end-to-end pipeline. The improvements in F1 score and accuracy, detailed further in this section, substantiate our hypothesis that integrating synthetic accelerometer data and optimizing the feature extraction process can significantly enhance HAR performance.

% ------------------------------------------------------- %

\begin{figure*}[!t]
  \centering
  % Adjust the vertical space as needed
  % \vspace{-10pt}
  % First image
  \includegraphics[width=0.49\textwidth]{Thesis_Template_DE_EN/BA_MA_English/Figures/scen1-sensor.png}
  % Optional: Adjust the horizontal space between the images
  \hspace*{\fill} 
  % Second image
  \includegraphics[width=0.49\textwidth]{Thesis_Template_DE_EN/BA_MA_English/Figures/scen2-sensor.png}
  \caption{Qualitative Comparison of Regression Models. This figure contrasts the performance of a regression model trained within the end-to-end pipeline (right) against one trained independently (left). The real IMU accelerometer data is represented in blue, while the predictions from the regression model trained in the end-to-end pipeline are depicted in red. Predictions from the independently trained regression model are shown in orange. Both visualizations are based on identical time windows for a direct comparison.}
  \label{fig:side-by-side}
  % Adjust the vertical space as needed
  % \vspace{-10pt}
\end{figure*}

\section{Results}
\begin{table}[!t]
  \centering
  \caption{Comparison of Method Performance for MM-fit}
  \label{tab:method_comparison_mmfit}
  \begin{tabular}{lcc}
    \toprule
    Method & F1 Score & Accuracy \\
    \midrule
    IMUTube \cite{kwon2020imutube}  &  $0.7697  \pm 0.0019$
   &   $0.8981 \pm 0.0019$
    \\
    \midrule
    Baseline (real data) & $0.9025 \pm 0.0323$ & $0.9559 \pm 0.0125$ \\
    Regression-first & $0.8848 \pm 0.0183$ & $0.9490 \pm 0.0070$ \\
    end-to-end ($\beta = 10$) & $0.9131 \pm 0.0266$ & $0.9494 \pm 0.0065$ \\
    \textbf{end-to-end ($\beta = 0$)} & \textbf{$0.9196 \pm 0.0038$} & \textbf{$0.9618 \pm 0.0017$} \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[!t]
  \centering
  \caption{Comparison of Method Performance on the UTD-MHAD \cite{chen2015utd}. Results marked with * are reported performance from the reference papers.}
  \label{tab:method_comparison_mhad}
  \begin{tabular}{lcc}
    \toprule
    Method & F1 Score & Accuracy \\
    \midrule
    Chen et al. \cite{chen2015utd} * & - & 0.6720 \\
    Memmesheimer et al. \cite{memmesheimer2020gimme} *& - & 0.7286 \\
    \midrule
    Baseline (real data) & $0.6285 \pm 0.0171$ & $0.6702 \pm 0.0137$ \\
    Regression-first & $0.6650 \pm 0.0204$ & $0.6911 \pm 0.0121$ \\
    end-to-end ($\beta = 10$) & $0.7342 \pm 0.0131$ & $0.7581 \pm 0.0020$ \\
    \textbf{end-to-end ($\beta = 0$)} & \textbf{0.7388} $\pm$ \textbf{0.0101} & \textbf{0.7635} $\pm$ \textbf{0.0081} \\
    \textbf{Adversarial ($\lambda = 1$)} & \textbf{0.7623} $\pm$ \textbf{0.033} & \textbf{0.7812} $\pm$ \textbf{0.0433} \\
    \bottomrule
  \end{tabular}
\end{table}

This section delves into the comparative analysis of the proposed method against the baselines. The discussion centers around the implications of the findings in terms of F1 score and accuracy, providing insights into the efficacy of synthetic data in training models for Human Activity Recognition (HAR).

\textbf{Baseline Methods Comparison:}
As we can see in \cref{tab:method_comparison_mmfit} and \cref{tab:method_comparison_mhad}, the end-to-end pipeline approach provides improvements in F1 scores and accuracy metrics, which strongly suggests that the integration of synthetic accelerometer data within the training process substantially enhances the performance of both the feature extraction and classification models. This observation is particularly pronounced in the context of our end-to-end pipeline, which seamlessly incorporates synthetic data alongside real sensor inputs. 

\begin{table}[!t]
  \centering
  \caption{Comparison of Sensor Generation Quality for MM-Fit}
  \label{tab:regression_comparison_MMFit}
  \begin{tabular}{lcc}
    \toprule
    Regression Training & MSE in the Test set \\
    \midrule
    Regression-first & $0.4890 \pm 0.0123$  \\
    \textbf{End-to-end pipeline} & \textbf{$0.4620 \pm 0.0055$} \\
    \bottomrule
  \end{tabular}
  % \vspace{-5mm}
\end{table}

\begin{table}[!t]
  \centering
  \caption{Comparison of Sensor Generation Quality for UTD-MHAD \cite{chen2015utd}}
  \label{tab:regression_comparison_MHAD}
  \begin{tabular}{lcc}
    \toprule
    Regression Training & MSE in the Test set \\
    \midrule
    Regression-first & $0.3910 \pm 0.0134$  \\
    \textbf{End-to-end pipeline} & \textbf{$0.3281 \pm 0.0122$} \\
    \bottomrule
  \end{tabular}
  % \vspace{-5mm}
\end{table}

\textbf{Proposed and Regression-first Methods Comparison:}
In addition to juxtaposing the proposed method with the baseline, we explored an alternative scenario wherein the regression model is first trained independently to generate synthetic accelerometer data, which is then used alongside real data to train the feature extraction and classification models. This sequential approach, while theoretically sound, did not yield the same level of improvement as the integrated end-to-end pipeline. In fact, it is interesting to notice that performing the regression first did not improve results when compared to the baseline. In our tests we have access to the full training set and thus it is possible that the regression model alone could not generate simulated data of sufficient quality. The discrepancy (see again \cref{tab:method_comparison_mmfit,tab:method_comparison_mhad}) in performance can be attributed to the dynamic feedback loop established in the end-to-end training process, where the simultaneous adaptation of the regression model and the classification framework to each other's outputs fosters a more synergistic learning environment. This interdependence ensures that the synthetic data is not only accurate but also optimally aligned with the objectives of the feature extraction and classification tasks. 

This gap can be seen quantitatively when comparing the MSE of the generated data in the test set. As we can see in \cref{tab:regression_comparison_MMFit,tab:regression_comparison_MHAD}, our regression model provided a smaller mean MSE in the test set along with a smaller standard deviation. We can also see qualitatively in \cref{fig:side-by-side} that our approach provides better coverage of high-frequency components in the signal, even if this increases the overall noise in the signal. 

\textbf{end-to-end and adversarial scheme comparison} 
Proposed method demonstrates improved performance over end-to-end pipeline, as shown in \cref{tab:method_comparison_mhad}. The use of an adversarial scheme encourages the feature extractor and pose-to-sensor network to produce higher-quality data, while the concurrent training with the activity classifier enhances performance on both the training set and unseen data.

\textbf{end-to-end and Existing Methods Comparison:}
We conducted a comparative analysis between our method and IMUTube \cite{kwon2020imutube} on the MM-Fit dataset. Given that our method generates sensor data using the original MM-Fit dataset exclusively, we followed the same protocol as IMUTube. Specifically, we utilized the pipeline outlined in \cite{kwon2020imutube} to create simulated accelerometer data for the left wrist using the original MM-Fit videos. This serves as a baseline measure of the quality of simulated data for this dataset, as IMUTube is not explicitly designed to optimize the quality of generated sensor data, unlike our regression model. Following the acquisition and calibration steps outlined in \cite{kwon2020imutube}, we trained our baseline model using a combination of half real sensor data and half IMUTube-generated data for each batch.

As depicted in \cref{tab:method_comparison_mmfit}, our proposed method outperforms IMUTube, which performs worse than the baseline. This is consistent with our previous findings: the quality of simulated data affects classification performance and, therefore, it is reasonable that simply applying IMUTube to the videos from the dataset itself only degrades the classifier performance. This can be overcome if one is using external videos to obtain additional data or, as is the case for our method, there is end-to-end optimization of sensor generation and activity recognition.

Regarding proposed method, it is important to note that the improvement of adversarial learning over end-to-end-pipeline was primarily observed in a limited subset of activities, due to the reliance on the secondary dataset, which may not cover the full range of activities as comprehensively as the primary dataset.

In addition, we evaluated the proposed method against existing methods, Chen et al. \cite{chen2015utd} and Memmesheimer et al. \cite{memmesheimer2020gimme}, on the UTD-MHAD dataset. As shown in \cref{tab:method_comparison_mhad}, the results by the existing methods were reported performance from the references, and the proposed method outperformed the existing methods. 

\textbf{Implications and Future Directions:}
The findings from this study underscore the critical role of synthetic data in enhancing HAR systems, particularly in scenarios plagued by the scarcity of labeled datasets. The end-to-end pipeline proposed herein not only demonstrates the feasibility of such an approach but also sets a new benchmark for the integration of synthetic and real data in training sophisticated machine learning models. However, it is important to acknowledge that the accuracy of the generated sensor data is inherently tied to the quality of the input pose sequences. In this work, we utilized the pose sequences provided by the open-access benchmark dataset; however, in real-world settings, accurate 3D pose estimation techniques are crucial and can significantly impact the quality of the generated sensor data. Thus, in future work, we plan to explore more robust 3D pose estimation techniques and conduct sensitivity analyses to evaluate the robustness of the proposed method to variations in pose estimation accuracy.