\chapter{Discussion}
\label{chap:discussion}

This chapter interprets the experimental findings, situates them within the broader literature, acknowledges limitations, and provides practical guidance for applying pose-to-IMU simulation in HAR systems.

\section{Key Findings}
\label{sec:key-findings}

\subsection{Effectiveness of Joint Training}

The central finding of this work is that joint end-to-end training of the pose-to-IMU regressor and activity classifier consistently outperforms decoupled approaches. This improvement stems from two complementary mechanisms:

\textbf{Task-aware simulation.} When the regressor receives gradients from the classification loss, it learns to produce simulated signals that are not merely similar to real accelerometer data, but specifically optimized for activity discrimination. The regressor may sacrifice perfect signal reconstruction in favor of preserving or amplifying activity-relevant features.

\textbf{Domain-robust feature learning.} By training the feature extractor on both real and simulated data simultaneously, the network learns representations that generalize across the domain gap. This is particularly valuable when test-time data may exhibit characteristics not fully captured in the training distribution.

\subsection{Role of Loss Components}

The three-term objective ($\alpha \mathcal{L}_{\text{cls}} + \beta \mathcal{L}_{\text{similarity}} + \gamma \mathcal{L}_{\text{regression}}$) provides complementary supervision signals:

\textbf{Classification loss ($\alpha$).} Essential for task performance. Training on both real and simulated paths exposes the classifier to greater variation, improving robustness. The equal weighting of real and simulated classification losses ensures neither domain dominates optimization. With separate classifiers (Scenario 2.4), \Cref{fig:scenario23-f1} confirms that both the real and simulated paths learn effective decision boundaries, with the simulated classifier achieving comparable F1 despite never seeing real accelerometer data during training.

\textbf{Feature similarity loss ($\beta$).} Provides explicit domain alignment at the representation level. The importance of this loss depends on the architecture: with shared feature extractors, the architecture itself provides implicit alignment, making explicit similarity loss a form of additional regularization. With separate feature extractors (Scenario 2.5), the similarity loss becomes critical for encouraging compatible representations---\Cref{fig:scenario24-confidence} shows that with proper $\beta$ tuning, the shared classifier receives features of similar confidence from both extractors.

\textbf{Regression loss ($\gamma$).} Grounds the simulated signals in physical reality. The ablation with $\gamma = 0$ (Scenario 2.2) demonstrates catastrophically that task gradients alone cannot guide meaningful simulation---without MSE supervision, the regressor produces chaotic, high-frequency noise bearing no resemblance to real accelerometer signals (\Cref{fig:scenario22-waveforms}). Including regression loss provides interpretable intermediate outputs and improves training stability by giving the regressor direct supervision early in training.

\subsection{When Does Simulation Help?}

Our analysis reveals that simulation-augmented training provides the greatest benefit in specific circumstances:

\textbf{Limited real data.} When the primary dataset is small (e.g., UTD-MHAD with 4 training subjects), simulated data provides meaningful augmentation that improves generalization. The benefit is less pronounced for larger datasets where real data already provides sufficient coverage.

\textbf{High pose-accelerometer correlation.} Activities where arm motion directly determines accelerometer readings (e.g., arm raises, bicep curls) benefit more than activities where accelerometer signal depends on subtle factors not captured by pose (e.g., wrist rotations, impact forces).

\textbf{Cross-subject generalization.} The domain gap between subjects is partially bridged by simulated data, which provides examples of the same activities with different execution styles derived from pose variation.

Conversely, simulation may provide limited benefit or even hurt performance when:
\begin{itemize}
    \item Pose estimation quality is poor, introducing noise that degrades the regressor
    \item The simulated signals systematically differ from real signals in ways that confuse the classifier
    \item The computational overhead of the simulation path outweighs marginal accuracy gains
\end{itemize}

\section{Adversarial Learning Insights}
\label{sec:adversarial-insights}

\subsection{Generator-Discriminator Dynamics}

Adversarial training introduces a competitive dynamic between the discriminator (seeking to distinguish real from simulated) and the generator components (seeking to fool the discriminator). Successful training requires careful balance, though the dynamics differ between our two adversarial approaches:

\textbf{Discriminator domination.} If the discriminator is too powerful (high capacity, low regularization), it maintains perfect classification throughout training. The generator receives weak gradients and fails to improve. Symptoms include discriminator accuracy remaining near 100\% and no reduction in feature distance.

\textbf{Generator domination.} If the discriminator is too weak, it provides uninformative gradients. The generator has no incentive to produce realistic features. Symptoms include discriminator accuracy immediately dropping to 50\% and features that satisfy the discriminator but fail downstream tasks.

\textbf{Achieving equilibrium.} The two adversarial scenarios use different mechanisms to achieve balance:
\begin{itemize}
    \item \textbf{Feature-level (GRL):} The lambda schedule starts with weak adversarial gradients (allowing initial feature learning) and increases strength as training progresses. As shown in \Cref{fig:scenario4-dynamics}, healthy training shows discriminator accuracy hovering around 50\% once GRL $\lambda$ ramps up, indicating the feature extractor successfully produces domain-invariant representations.
    \item \textbf{Signal-level (WGAN-GP):} Alternating updates with $n_{\text{critic}}$ discriminator steps per generator step naturally balances the two. The gradient penalty enforces the Lipschitz constraint, preventing discriminator collapse. As shown in \Cref{fig:scenario42-dynamics}, staged training with MSE pretraining (before the dashed vertical line) establishes reasonable signal structure before adversarial feedback begins.
\end{itemize}

\subsection{Feature-Level vs. Signal-Level Discrimination}

Our experiments compare two discriminator operating points with different training strategies:

\textbf{Feature-level discrimination (Scenario 4.1)} operates on embeddings $\mathbf{z}_{\text{real}}$ vs. $\mathbf{z}_{\text{sim}}$ using GRL with binary cross-entropy loss. The adversarial gradient flows primarily to the feature extractor $F$, encouraging domain-invariant representations. This approach aligns with domain adaptation methods like DANN \cite{ganin2016domain} and enables end-to-end training in a single backward pass.

\textbf{Signal-level discrimination (Scenario 4.2)} operates on raw accelerometer signals $\mathbf{a}$ vs. $\tilde{\mathbf{a}}$ using WGAN-GP with alternating updates. The adversarial gradient flows to the regressor $R$, encouraging realistic signal synthesis. This approach aligns with generative adversarial networks for time series, and the Wasserstein distance provides smoother gradients than BCE when distributions have limited overlap. An optional ACGAN variant adds class conditioning to encourage activity-specific realism.

The two approaches target different components: feature-level discrimination directly improves the representations used for classification, while signal-level discrimination improves regressor output quality. These are complementary goals---feature-level alignment may compensate for imperfect simulation, while signal-level realism reduces the burden on the feature extractor to bridge the domain gap.

Notably, both approaches achieve their objective of fooling the discriminator (D accuracy $\approx 50\%$), yet only signal-level discrimination on UTD-MHAD translates to improved classification (+3.3\%). This reveals an important insight: \emph{domain alignment is necessary but not sufficient} for downstream task improvement. The feature-level approach may discard task-relevant information in pursuit of domain invariance (\Cref{tab:adversarial-comparison}).

\subsection{Connection to Domain Adaptation Literature}

Our adversarial training scenarios implement ideas from unsupervised domain adaptation, applied to the sim-to-real gap:

\textbf{Domain-Adversarial Neural Networks (DANN)} \cite{ganin2016domain} introduced the gradient reversal layer for end-to-end domain adaptation. Our feature-level discriminator scenario directly implements this approach, treating real accelerometer data as the target domain and simulated data as the source domain.

\textbf{Adversarial Discriminative Domain Adaptation (ADDA)} \cite{tzeng2017adversarial} uses separate encoders for source and target domains with adversarial alignment. Our separate feature extractor scenario ($F$ and $F_{\text{sim}}$) combined with adversarial training approximates this approach.

The key difference from standard domain adaptation is that we have \emph{paired} data: each pose sequence has a corresponding real accelerometer signal. This enables the regression loss and feature similarity loss, which are unavailable in typical unsupervised domain adaptation settings.

\subsection{Adversarial Loss Function Choice}

Our implementation uses different adversarial objectives for the two scenarios based on their requirements:

\textbf{Binary cross-entropy (feature-level).} For feature discrimination with GRL, BCE provides a straightforward objective that works well when combined with the gradient reversal mechanism. The feature space is relatively low-dimensional, and BCE gradients are sufficient for alignment.

\textbf{Wasserstein distance with gradient penalty (signal-level).} For signal discrimination, we use WGAN-GP \cite{gulrajani2017improved} which provides smoother gradients when real and simulated signal distributions have limited overlap. The gradient penalty ($\lambda_{\text{GP}} = 10$) enforces the Lipschitz constraint without weight clipping, enabling more stable training. This choice is motivated by the high-dimensional, structured nature of accelerometer signals where BCE can suffer from vanishing gradients.

Alternative formulations such as least-squares loss \cite{mao2017least} or hinge loss could be explored for either scenario, though our experiments found the BCE/WGAN-GP pairing effective for their respective use cases.

\section{Limitations}
\label{sec:limitations}

\subsection{Dataset-Specific Tuning}

A significant limitation of our approach is the need for dataset-specific hyperparameter optimization. The optimal loss weights, learning rates, and architecture configurations differ substantially between UTD-MHAD and MM-Fit. This suggests that practitioners cannot simply apply a single ``best'' configuration to new datasets.

The dataset dependence likely arises from differences in:
\begin{itemize}
    \item Sampling rates (50 Hz vs. 100 Hz)
    \item Activity characteristics (discrete actions vs. continuous exercises)
    \item Pose-accelerometer correlation (sensor placement, skeleton quality)
    \item Dataset size (training set variability)
\end{itemize}

\subsection{Computational Cost}

The multi-pass HPO strategy, while effective, requires substantial computational resources:
\begin{itemize}
    \item Each HPO pass runs 20--50 trials
    \item Each trial requires training for 30--100 epochs
    \item Top-K validation multiplies evaluation cost by the number of seeds
\end{itemize}

For resource-constrained settings, practitioners may need to adopt simplified search strategies or transfer hyperparameters from similar datasets.

\subsection{Skeleton Quality Dependence}

Our method assumes access to reasonably accurate 3D skeleton pose data. In practice, pose estimation from video introduces errors:
\begin{itemize}
    \item Joint position jitter from frame-to-frame estimation noise
    \item Occlusion-induced artifacts when body parts are not visible
    \item Scale ambiguity in monocular 3D pose estimation
    \item Temporal misalignment between estimated pose and sensor data
\end{itemize}

When skeleton quality degrades, the regressor receives noisy input, potentially producing simulated signals that harm rather than help classification. Our experiments use ground-truth or high-quality estimated poses; performance on lower-quality pose estimates remains to be evaluated.

\subsection{Sensor Placement Generalization}

Our method learns a mapping from arm joint poses to wrist-worn accelerometer signals. This mapping is specific to the sensor placement location. Generalizing to:
\begin{itemize}
    \item Different body locations (e.g., hip, ankle)
    \item Different sensor orientations
    \item Multiple simultaneous sensors
\end{itemize}
would require retraining or architectural modifications. The physics of accelerometer signals depends heavily on the kinematic chain from joints to sensor, which changes with placement.

\subsection{Activity Class Coverage}

The auxiliary data experiments with NTU RGB+D reveal that secondary datasets are most beneficial when they share activity semantics with the primary dataset. When activity classes differ substantially, the regressor may learn mappings that do not transfer. The per-class analysis in \Cref{fig:scenario3-perclass} shows this effect clearly: activities with NTU equivalents tend to benefit, while dataset-specific activities (e.g., MM-Fit's fitness exercises) may degrade. This limits the utility of large-scale pose datasets for arbitrary HAR tasks.

Tables~\ref{tab:utd-ntu-overlap} and~\ref{tab:mmfit-ntu-overlap} show the activity class overlap between each primary dataset and NTU RGB+D. The limited overlap---particularly for MM-Fit's fitness-focused activities---helps explain why auxiliary NTU data provides inconsistent benefits across datasets.

\begin{table}[htbp]
\centering
\caption{UTD-MHAD activity classes and their overlap with NTU RGB+D. A checkmark ($\checkmark$) indicates a semantically similar activity exists in NTU.}
\label{tab:utd-ntu-overlap}
\begin{tabular}{llc}
\toprule
\textbf{UTD-MHAD Activity} & \textbf{NTU Equivalent} & \textbf{Overlap} \\
\midrule
Swipe Left & --- & $\times$ \\
Swipe Right & --- & $\times$ \\
Wave & Hand waving (A023) & $\checkmark$ \\
Clap & Clapping (A010) & $\checkmark$ \\
Throw & Throw (A007) & $\checkmark$ \\
Arm Cross & Cross hands in front (A040) & $\checkmark$ \\
Basketball Shoot & --- & $\times$ \\
Draw X & --- & $\times$ \\
Draw Circle (CW) & --- & $\times$ \\
Draw Circle (CCW) & --- & $\times$ \\
Draw Triangle & --- & $\times$ \\
Bowling & --- & $\times$ \\
Boxing & Punching/slapping (A050) & $\checkmark$ \\
Baseball Swing & --- & $\times$ \\
Tennis Swing & --- & $\times$ \\
Arm Curl & --- & $\times$ \\
Tennis Serve & --- & $\times$ \\
Push & Pushing other person (A052) & $\checkmark$ \\
Knock & --- & $\times$ \\
Catch & --- & $\times$ \\
Pickup \& Throw & Pickup (A006), Throw (A007) & $\checkmark$ \\
\midrule
\multicolumn{2}{r}{\textbf{Total overlap:}} & \textbf{7/21 (33\%)} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[htbp]
\centering
\caption{MM-Fit activity classes and their overlap with NTU RGB+D. Fitness exercises have minimal representation in NTU's daily activity vocabulary.}
\label{tab:mmfit-ntu-overlap}
\begin{tabular}{llc}
\toprule
\textbf{MM-Fit Activity} & \textbf{NTU Equivalent} & \textbf{Overlap} \\
\midrule
Squats & --- & $\times$ \\
Lunges & --- & $\times$ \\
Bicep Curls & --- & $\times$ \\
Situps & --- & $\times$ \\
Pushups & --- & $\times$ \\
Tricep Extensions & --- & $\times$ \\
Dumbbell Rows & --- & $\times$ \\
Jumping Jacks & Jump up (A027), Hopping (A026) & $\checkmark$ \\
Dumbbell Shoulder Press & --- & $\times$ \\
Lateral Shoulder Raises & --- & $\times$ \\
Non-activity & --- & $\times$ \\
\midrule
\multicolumn{2}{r}{\textbf{Total overlap:}} & \textbf{1/11 (9\%)} \\
\bottomrule
\end{tabular}
\end{table}

\section{Practical Recommendations}
\label{sec:recommendations}

Based on our experimental findings, we offer guidelines for practitioners considering pose-to-IMU simulation for HAR.

\subsection{When to Use Simulation-Augmented Training}

\textbf{Recommended when:}
\begin{itemize}
    \item Labeled accelerometer data is scarce (fewer than 5 training subjects)
    \item High-quality synchronized pose data is available
    \item Target activities involve clear arm/body motions visible in skeleton
    \item Cross-subject generalization is the primary goal
\end{itemize}

\textbf{Not recommended when:}
\begin{itemize}
    \item Abundant labeled accelerometer data already exists
    \item Pose estimation is unreliable or unavailable
    \item Activities depend on subtle motions not captured by skeleton (e.g., finger gestures)
    \item Computational resources are severely limited
\end{itemize}

\subsection{Scenario Selection}

For practitioners unsure which training scenario to adopt, we recommend the following decision process:

\begin{enumerate}
    \item \textbf{Start with the baseline.} The shared feature extractor and classifier configuration with all losses enabled ($\alpha, \beta, \gamma > 0$) provides a robust starting point.

    \item \textbf{Try removing MSE loss.} If classification performance is the sole objective and signal interpretability is not required, setting $\gamma = 0$ may improve results by allowing the regressor to focus on task-relevant features.

    \item \textbf{Consider adversarial training for small datasets.} When training data is very limited, the feature-level discriminator provides additional regularization that may improve generalization.

    \item \textbf{Use auxiliary data cautiously.} Secondary pose datasets help only when they share relevant motion patterns with the target task. Mismatched auxiliary data can hurt performance.
\end{enumerate}

\subsection{Hyperparameter Priorities}

Given limited HPO budget, prioritize tuning in this order:

\begin{enumerate}
    \item \textbf{Loss weights} ($\alpha$, $\beta$, $\gamma$): These have the largest impact on training dynamics. The ratio between weights matters more than absolute values.

    \item \textbf{Data parameters} (stride, batch size): Smaller strides provide more training samples; smaller batches may improve generalization through gradient noise.

    \item \textbf{Learning rate and regularization}: Standard deep learning tuning; learning rate warmup helps stability.

    \item \textbf{Architecture capacity}: Can often remain at defaults; tune only if overfitting or underfitting is evident.
\end{enumerate}

\subsection{Evaluation Best Practices}

To obtain reliable performance estimates:

\begin{itemize}
    \item Always use subject-disjoint splits for training, validation, and test sets
    \item Report results as mean $\pm$ std over multiple random seeds (minimum 5, preferably 10)
    \item Use validation set only for model selection; evaluate test set once with final configuration
    \item Use ablation studies (e.g., $\gamma=0$, $\beta=0$) to isolate the contribution of each loss component
\end{itemize}

\section{Broader Implications}
\label{sec:implications}

\subsection{Sim-to-Real Transfer in Sensor Systems}

Our work contributes to the broader challenge of sim-to-real transfer for sensor-based systems. While simulation is common in robotics and autonomous driving, its application to wearable sensing remains underexplored. The success of pose-to-IMU simulation suggests that similar approaches may benefit other sensor modalities:

\begin{itemize}
    \item \textbf{Gyroscope simulation:} Extending the regressor to predict angular velocity from pose derivatives
    \item \textbf{Pressure sensor simulation:} Predicting foot pressure from full-body pose for gait analysis
    \item \textbf{EMG simulation:} Predicting muscle activation patterns from biomechanical models
\end{itemize}

\subsection{Leveraging Video Data for Wearable AI}

A compelling use case for our method is leveraging the vast amount of existing video data (e.g., YouTube fitness videos, sports broadcasts) to train wearable AI systems. By extracting pose from video and simulating IMU signals, systems can learn from data that would otherwise require expensive sensor instrumentation to collect.

This paradigm shift---from ``collect sensor data'' to ``mine existing video''---could dramatically reduce the data collection burden for HAR research and enable rapid prototyping of new activity recognition applications.

\subsection{Multimodal Learning}

Our framework exemplifies multimodal learning where different modalities (pose, accelerometer) provide complementary supervision. The joint training approach could extend to other multimodal HAR settings:

\begin{itemize}
    \item Audio + accelerometer for activity recognition
    \item Video + IMU for action segmentation
    \item Multiple sensor locations with partial observability
\end{itemize}

The key insight---that auxiliary modalities can improve primary modality performance through joint training---generalizes beyond the specific pose-to-IMU case studied here.
